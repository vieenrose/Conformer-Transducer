{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vieenrose/Conformer-Transducer/blob/master/Bleat_Llama_2_function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bleat ðŸ¦™ðŸ˜¯\n",
        "## LLaMA 2 Function Calling\n",
        "\n",
        "Bleat allows you to enable function calling in LLaMA 2 is a similar fashion to OpenAI's implementation for ChatGPT"
      ],
      "metadata": {
        "id": "HSNCKEBIxiOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install requirements\n",
        "!nvidia-smi\n",
        "!pip install transformers peft colorama bitsandbytes"
      ],
      "metadata": {
        "id": "-7Va6A67y9qh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load models and code (this takes a while)\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer, StoppingCriteria, GenerationConfig\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import colorama\n",
        "\n",
        "base_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"IfanSnek/bleat-adapter\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"./\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "class Streamer:\n",
        "    def __init__(self, enabled):\n",
        "        self.do_stream = enabled\n",
        "\n",
        "    def put(self, input_ids):\n",
        "        if self.do_stream:\n",
        "            print(tokenizer.decode(input_ids[0]))\n",
        "\n",
        "    def end(self):\n",
        "        return\n",
        "\n",
        "\n",
        "class EndOfFunctionCriteria(StoppingCriteria):\n",
        "    \"\"\"Custom `StoppingCriteria` which checks if all generated functions in the batch are completed.\"\"\"\n",
        "\n",
        "    def __init__(self, start_length, eof_strings):\n",
        "        self.start_length = start_length\n",
        "        self.eof_strings = eof_strings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        \"\"\"Returns true if all generated sequences contain any of the end-of-function strings.\"\"\"\n",
        "        decoded_generation = self.tokenizer.decode(input_ids[0])\n",
        "        new_generation = decoded_generation[self.start_length:]\n",
        "        return any(\n",
        "            [\n",
        "                eof_string in new_generation[len(eof_string):]\n",
        "                for eof_string in self.eof_strings\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "config = GenerationConfig.from_model_config(base_model.config)\n",
        "config.max_length = 4096\n",
        "config.max_new_tokens = 128\n",
        "\n",
        "\n",
        "def infer(text):\n",
        "    \"\"\"\n",
        "    Infer a function call from a prompt\n",
        "    :param text: The prompt to infer from. Should not end with a header (eg, \"### System: Hi!\\n\" but not \"### System:\")\n",
        "    :return: The new text, and the header it starts with\n",
        "    \"\"\"\n",
        "\n",
        "    headers = [\"### System:\", \"### User:\", \"### Call:\", \"### Return:\", \"### Assistant:\"]\n",
        "    ids = base_model.generate(\n",
        "        input_ids=tokenizer.encode(text, return_tensors=\"pt\").to(base_model.device),\n",
        "        stopping_criteria=[EndOfFunctionCriteria(len(text), headers)],\n",
        "        generation_config=config,\n",
        "    )\n",
        "    decoded_generation = tokenizer.decode(ids[0])\n",
        "    new_generation = decoded_generation.replace(text, \"\").replace(\"<s>\", \"\").replace(\"<\\\\s>\", \"\").strip(\"\\n\").strip()\n",
        "\n",
        "    # Find the header the response starts with\n",
        "    start_header = \"\"\n",
        "    for header in headers:\n",
        "        if new_generation.startswith(header):\n",
        "            start_header = header\n",
        "            break\n",
        "\n",
        "    if start_header != \"\":\n",
        "        new_generation = new_generation[len(start_header):]\n",
        "\n",
        "    # Find the header the response ends with\n",
        "    for header in headers:\n",
        "        if new_generation.endswith(header):\n",
        "            new_generation = new_generation[:-len(header)]\n",
        "\n",
        "    return new_generation, start_header\n",
        "\n",
        "\n",
        "functions = []\n",
        "previous_functions = []\n",
        "\n",
        "\n",
        "def register_function(json_definition, fn):\n",
        "    functions.append((json_definition, fn))\n",
        "\n",
        "\n",
        "running_chat = \"\"\n",
        "\n",
        "\n",
        "def query(prompt):\n",
        "    \"\"\"\n",
        "    Main function for querying the assistant\n",
        "    :param prompt: The prompt to query with\n",
        "    \"\"\"\n",
        "    global running_chat\n",
        "    global functions\n",
        "    global previous_functions\n",
        "\n",
        "    if prompt == \"reset\":\n",
        "        running_chat = \"\"\n",
        "        previous_functions = []\n",
        "        return \"Resetting chat\"\n",
        "\n",
        "    prompt = \"### User: \" + prompt + \"\\n\"\n",
        "\n",
        "    # If functions changed, create a new system text\n",
        "    if functions != previous_functions:\n",
        "        system_text = \"### System:\\n\" + json.dumps([f[0] for f in functions], indent=4) + \"\\n\"\n",
        "        running_chat += system_text\n",
        "        previous_functions = functions\n",
        "\n",
        "    # Generate the next response\n",
        "    output, role = infer(running_chat + prompt)\n",
        "\n",
        "    # If the response is a function call, try to call it\n",
        "    if role == \"### Call:\":\n",
        "        try:\n",
        "            call = json.loads(output)\n",
        "            if \"name\" not in call or \"parameters\" not in call:\n",
        "                raise ValueError\n",
        "        except ValueError:\n",
        "            print(colorama.Fore.RED + \"Error parsing call: \" + output + colorama.Style.RESET_ALL)\n",
        "            response = \"Assistant: The call I wrote was formatted wrong. Please try again.\\n\"\n",
        "            running_chat += response\n",
        "            return response\n",
        "\n",
        "        # Check if the function exists\n",
        "        if call[\"name\"] not in [f[0][\"name\"] for f in functions]:\n",
        "            print(colorama.Fore.RED + \"Error: function \" + call[\"name\"] + \" does not exist\" + colorama.Style.RESET_ALL)\n",
        "            response = \"### Assistant: The function I wrote does not exist. Please try again.\\n\"\n",
        "            running_chat += response\n",
        "            return response\n",
        "\n",
        "        # Try to call the function\n",
        "        try:\n",
        "            fn = [f[1] for f in functions if f[0][\"name\"] == call[\"name\"]][0]\n",
        "            params = call[\"parameters\"].values()\n",
        "\n",
        "            fn_output = str(fn(*params))\n",
        "            params_str = \", \".join([str(p) for p in params])\n",
        "            print(f\"{colorama.Fore.GREEN}{call['name']}({params_str}) = {fn_output}{colorama.Style.RESET_ALL}\")\n",
        "\n",
        "            fn_output = {\n",
        "                \"result\": fn_output,\n",
        "            }\n",
        "            fn_output = json.dumps(fn_output)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(colorama.Fore.RED + \"Error calling function: \" + str(e) + \"\\n\" + str(call) + colorama.Style.RESET_ALL)\n",
        "            response = \"### Assistant: The function tried to call had an error. Please try again.\\n\"\n",
        "            running_chat += response\n",
        "            return response\n",
        "\n",
        "        # Feed the return value back into the model\n",
        "        running_chat += \"### Call:\\n\" + json.dumps(call) + \"\\n\"\n",
        "        running_chat += \"### Return:\\n\" + fn_output + \"\\n\"\n",
        "        output, role = infer(running_chat)\n",
        "\n",
        "    # Add the response to the running chat\n",
        "    running_chat += \"### Assistant:\" + output + \"\\n\"\n",
        "\n",
        "    return output.strip()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "unR0m365NiW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, define a function in the cell below:"
      ],
      "metadata": {
        "id": "uAtDZFhMy99r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2Y53WLMsnVO"
      },
      "outputs": [],
      "source": [
        "def sunset(city):\n",
        "  return \"6:00 PM\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have run the cell, create a definition in JSON like so:\n",
        "\n",
        "\n",
        "```\n",
        "definition = {\n",
        "  \"name\": \"function_name\",\n",
        "  \"description\": \"Function Description\",\n",
        "  \"parameters\": [\n",
        "      \"parameter\": {\n",
        "          \"type\": \"python type (str, int, etc.)\",\n",
        "          \"description\": \"Parameter description\"\n",
        "      },\n",
        "  ]\n",
        "  \"required\": [\"list\", \"of\", \"required\", \"parameters\"]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7VifESCIyATf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "definition = {\n",
        "        \"name\": \"get_sunset_time\",\n",
        "        \"description\": \"Get the time of sunset in a city\",\n",
        "        \"parameters\": [\n",
        "            {\n",
        "                \"name\": \"city\",\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "        ],\n",
        "        \"required\": [\"city\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "09jJHWN9yGp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, register the function."
      ],
      "metadata": {
        "id": "peHNqUonyXYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear old functions if applicable\n",
        "functions = []\n",
        "previous_functions = []\n",
        "\n",
        "register_function(definition, sunset)"
      ],
      "metadata": {
        "id": "BblaG6hbyUd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send a message to LLaMA:"
      ],
      "metadata": {
        "id": "sScEXSHJycrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"reset\") # Forgets everything\n",
        "print(query(\"When does the sun set in Bostonr?\"))"
      ],
      "metadata": {
        "id": "6WG7oIEzyiNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebb9bd1-32a5-4e99-e0dd-db3c12192d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mget_sunset_time(Denver) = 6:00 PM\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a whole heckin' conversation:"
      ],
      "metadata": {
        "id": "49TQnZaZywCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"reset\")\n",
        "while True:\n",
        "  print(query(input(\"> \")))"
      ],
      "metadata": {
        "id": "67c0Ki9Cy22G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}